Revision: 2353fa6a306f74f599e162a46616bc3378124278
Patch-set: 2
File: liblog/logd_write.c

222:20-222:33
Mon Feb 16 14:44:06 2015 +0000
Author: John Michelau <1005146@85c56323-6fa9-3386-8a01-6480fb634889>
UUID: 2b17e08c_c53e5a02
Bytes: 420
Only logging this into the events stream eliminates something we found valuable.  Our perspective is that of somebody analyzing the content of the logs.  If I see log message A in thread X, then later see log message C in thread X, and I expected to see log message B in thread X in between, but didn't, I want to see the "Dropped X" messages for thread X, so that I can distinguish between a log drop and an actual bug.

222:20-222:33
Mon Feb 16 14:46:03 2015 +0000
Author: John Michelau <1005146@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 2b17e08c_c53e5a02
UUID: 2b17e08c_08c887ac
Bytes: 239
Hmm, I see your comments about using 'logcat -d -b main -b events' to do the interleaving.  The problem for us is that we typically use logs from our persistent reader collected passively by other users, and the streams aren't interleaved.

222:20-222:33
Mon Feb 23 16:03:51 2015 +0000
Author: Mark Salyzyn <1032276@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 2b17e08c_08c887ac
UUID: 8885328c_406d8e93
Bytes: 494
Interleaving is somewhat specific to your re-logging infrastructure. I will look into an adjustment to logd/logcat (as hinted above) that will help address your concerns over interleaving.

Your logging infrastructure breaks our security concerns over PII by storing the data in persistent files. What really needs to be done is that the bugreport mechanism needs to be adjusted to address your needs. Please advise on the shortcomings you have with bugreports so we may fix it to work for you.

222:20-222:33
Mon Feb 23 16:03:51 2015 +0000
Author: Mark Salyzyn <1032276@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 2b17e08c_c53e5a02
UUID: a880367a_01654a7f
Bytes: 1283
I understand. Please be advised that your reaction is a result of FUD associated with a _bug_ that you had thankfully resolved in logd; hopefully never to bite us again.

I am not comfortable with creating a statistical infrastructure to record TID and LogID for each associated log drop, then play them back. The goal is to re-establish trust, I am sorry I broke that trust. But I still need/want an inexpensive mechanism to report logging failures if they occur in order to take corrective action on the platform. This message is to be considered 'fatal' from the logging perspective and I need to make that more prominent to the observer is my take (I will think about that). My gut tell me that the event needs to be special-cased in logd or logcat ...

As for being able to continue to triage a problem in the face of data-loss, almost all scenarios one can comes up with will not benefit triage with a more specific loss statistic. The reaction should be to fix the platform problem (hopefully one line in init.rc, and not fix a bug in the logger) and stop limping along.

NB: putting 10000 into /proc/sys/net/unix/dgram_max_qlen and restarting logd can be done without adjusting the platform init.rc file. That should have _bypassed_ the bug in logd that was fixed by your CL.

219:0-240:5
Fri Feb 13 00:39:47 2015 +0000
Author: Mark Salyzyn <1032276@85c56323-6fa9-3386-8a01-6480fb634889>
UUID: 6b447857_af749932
Bytes: 428
'dropped' can get damaged by multiple threads feeding log messages at the same time. We can not afford to use locking as it adds considerable overhead. Explore means to locklessly(sic) improve the reliability of the 'dropped' value.

What does not get lost in the shuffle is that this event tells us messages were dropped, regardless if the number is correct or not. This piece of information is more valuable than the quantity.

219:0-240:5
Fri Feb 13 00:57:26 2015 +0000
Author: Wink Saville <1001401@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 6b447857_af749932
UUID: ab3af0c2_d45e5230
Bytes: 55
Can we use an atomic set here and and atomic inc below?

219:0-240:5
Fri Feb 13 02:52:56 2015 +0000
Author: Mark Salyzyn <1032276@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: ab3af0c2_d45e5230
UUID: cb5e442b_d0c9b45e
Bytes: 66
I was testing it already ... but decided to use atomic_add and inc

219:0-240:5
Mon Feb 16 14:44:06 2015 +0000
Author: John Michelau <1005146@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: cb5e442b_d0c9b45e
UUID: 6b2558ee_67905236
Bytes: 259
My original proposal used __thread to avoid this, which is a GNU extension.  It looks like C++11 has a standard mechanism (thread_local), but I didn't try it.  I guess if you're using atomic on a normal static then you don't care about thread-specific counts.

219:0-240:5
Mon Feb 16 14:52:38 2015 +0000
Author: John Michelau <1005146@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 6b2558ee_67905236
UUID: 6b2558ee_a7a9aa6c
Bytes: 256
This says nothing about performance impacts, though.  If I recall correctly from our rough profiling, we were looking at a penalty of about 80 ns on one of our low-tier devices (Moto E).  At the time, we figured it was worth it for at least our dev builds.

310:8-310:18
Fri Feb 13 00:39:47 2015 +0000
Author: Mark Salyzyn <1032276@85c56323-6fa9-3386-8a01-6480fb634889>
UUID: 2b2600b6_d07782b6
Bytes: 29
We should limit to INT32_MAX/

310:8-310:18
Mon Feb 16 14:44:06 2015 +0000
Author: John Michelau <1005146@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 2b2600b6_d07782b6
UUID: 2b17e08c_e541d67c
Bytes: 225
We went with only 8 bits, since we figured past 255 drops the precision just wasn't interesting anymore.  Our perspective is that of somebody analyzing the log content, though, not profiling the system for logging throughput.

