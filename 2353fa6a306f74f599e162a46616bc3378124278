Revision: 2353fa6a306f74f599e162a46616bc3378124278
Patch-set: 2
File: liblog/logd_write.c

222:20-222:33
Mon Feb 16 14:44:06 2015 +0000
Author: John Michelau <1005146@85c56323-6fa9-3386-8a01-6480fb634889>
UUID: 2b17e08c_c53e5a02
Bytes: 420
Only logging this into the events stream eliminates something we found valuable.  Our perspective is that of somebody analyzing the content of the logs.  If I see log message A in thread X, then later see log message C in thread X, and I expected to see log message B in thread X in between, but didn't, I want to see the "Dropped X" messages for thread X, so that I can distinguish between a log drop and an actual bug.

222:20-222:33
Mon Feb 16 14:46:03 2015 +0000
Author: John Michelau <1005146@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 2b17e08c_c53e5a02
UUID: 2b17e08c_08c887ac
Bytes: 239
Hmm, I see your comments about using 'logcat -d -b main -b events' to do the interleaving.  The problem for us is that we typically use logs from our persistent reader collected passively by other users, and the streams aren't interleaved.

222:20-222:33
Mon Feb 23 16:03:51 2015 +0000
Author: Mark Salyzyn <1032276@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 2b17e08c_08c887ac
UUID: 8885328c_406d8e93
Bytes: 494
Interleaving is somewhat specific to your re-logging infrastructure. I will look into an adjustment to logd/logcat (as hinted above) that will help address your concerns over interleaving.

Your logging infrastructure breaks our security concerns over PII by storing the data in persistent files. What really needs to be done is that the bugreport mechanism needs to be adjusted to address your needs. Please advise on the shortcomings you have with bugreports so we may fix it to work for you.

222:20-222:33
Mon Feb 23 16:03:51 2015 +0000
Author: Mark Salyzyn <1032276@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 2b17e08c_c53e5a02
UUID: a880367a_01654a7f
Bytes: 1283
I understand. Please be advised that your reaction is a result of FUD associated with a _bug_ that you had thankfully resolved in logd; hopefully never to bite us again.

I am not comfortable with creating a statistical infrastructure to record TID and LogID for each associated log drop, then play them back. The goal is to re-establish trust, I am sorry I broke that trust. But I still need/want an inexpensive mechanism to report logging failures if they occur in order to take corrective action on the platform. This message is to be considered 'fatal' from the logging perspective and I need to make that more prominent to the observer is my take (I will think about that). My gut tell me that the event needs to be special-cased in logd or logcat ...

As for being able to continue to triage a problem in the face of data-loss, almost all scenarios one can comes up with will not benefit triage with a more specific loss statistic. The reaction should be to fix the platform problem (hopefully one line in init.rc, and not fix a bug in the logger) and stop limping along.

NB: putting 10000 into /proc/sys/net/unix/dgram_max_qlen and restarting logd can be done without adjusting the platform init.rc file. That should have _bypassed_ the bug in logd that was fixed by your CL.

222:20-222:33
Wed Mar 04 21:55:47 2015 +0000
Author: John Michelau <1005146@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: a880367a_01654a7f
UUID: f25ea2fc_98fb95fd
Bytes: 639
Understood and agree about the FUD.

Making the failure more prominent is exactly what I'd like as well.  Absent some automated monitoring of the event log stream in all test setups (both passive and active), we're relying on those actually viewing the logs to let us know that it's broken.  The automated monitoring itself wouldn't really help in the case of a single user passing some logs along ad-hoc, but as you say maybe that's just the FUD talking.

I suppose adding a smoke test to our CI that introduces some stress and then looks for the issue in the event stream would be a good thing regardless.

Thanks for the effort on this.

222:20-222:33
Wed Mar 04 21:55:47 2015 +0000
Author: John Michelau <1005146@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 8885328c_406d8e93
UUID: f25ea2fc_3863698f
Bytes: 422
Bugreports work well for certain situations, but are too large of a hammer for many of the things we do, and too small of one for others.  I don't know how you could make them work well in all situations.  The RAM-based log buffers don't have enough depth for solving network-related issues or other things that manifest over time.

As for PII, we don't log to persistent files on consumer devices.  We use it for testing.

222:20-222:33
Wed Mar 04 22:04:21 2015 +0000
Author: Mark Salyzyn <1032276@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: f25ea2fc_3863698f
UUID: f25ea2fc_1bd3ff55
Bytes: 413
If you dogfood test with ro.logd.size=4M (or for example ro.logd.main.size=4M for _one_ log buffer) set in init.rc for eng or userdebug builds you may have that large, at least one day long, log buffer you are looking for, with regards to tracking network-related issues. 32M on all buffers would give you a week of coverage; but at the cost of OOM killer ...

bugreports would be very unhappy if you did this ;-}

222:20-222:33
Wed Mar 04 22:13:00 2015 +0000
Author: John Michelau <1005146@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: f25ea2fc_1bd3ff55
UUID: 526daeb2_f3c750dd
Bytes: 347
Yeah, that's the problem, although I can't really say what I think on this topic without publicly bashing RIL vendors.  Cranking up the RAM buffer size doesn't work for Dogfooding, because it invalidates a bunch of others things we're monitoring.  A true Tragedy of the (Dogfood) Commons... and an inherent flaw in on-target diagnostics.  Oh well.

219:0-240:5
Fri Feb 13 00:39:47 2015 +0000
Author: Mark Salyzyn <1032276@85c56323-6fa9-3386-8a01-6480fb634889>
UUID: 6b447857_af749932
Bytes: 428
'dropped' can get damaged by multiple threads feeding log messages at the same time. We can not afford to use locking as it adds considerable overhead. Explore means to locklessly(sic) improve the reliability of the 'dropped' value.

What does not get lost in the shuffle is that this event tells us messages were dropped, regardless if the number is correct or not. This piece of information is more valuable than the quantity.

219:0-240:5
Fri Feb 13 00:57:26 2015 +0000
Author: Wink Saville <1001401@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 6b447857_af749932
UUID: ab3af0c2_d45e5230
Bytes: 55
Can we use an atomic set here and and atomic inc below?

219:0-240:5
Fri Feb 13 02:52:56 2015 +0000
Author: Mark Salyzyn <1032276@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: ab3af0c2_d45e5230
UUID: cb5e442b_d0c9b45e
Bytes: 66
I was testing it already ... but decided to use atomic_add and inc

219:0-240:5
Mon Feb 16 14:44:06 2015 +0000
Author: John Michelau <1005146@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: cb5e442b_d0c9b45e
UUID: 6b2558ee_67905236
Bytes: 259
My original proposal used __thread to avoid this, which is a GNU extension.  It looks like C++11 has a standard mechanism (thread_local), but I didn't try it.  I guess if you're using atomic on a normal static then you don't care about thread-specific counts.

219:0-240:5
Mon Feb 16 14:52:38 2015 +0000
Author: John Michelau <1005146@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 6b2558ee_67905236
UUID: 6b2558ee_a7a9aa6c
Bytes: 256
This says nothing about performance impacts, though.  If I recall correctly from our rough profiling, we were looking at a penalty of about 80 ns on one of our low-tier devices (Moto E).  At the time, we figured it was worth it for at least our dev builds.

310:8-310:18
Fri Feb 13 00:39:47 2015 +0000
Author: Mark Salyzyn <1032276@85c56323-6fa9-3386-8a01-6480fb634889>
UUID: 2b2600b6_d07782b6
Bytes: 29
We should limit to INT32_MAX/

310:8-310:18
Mon Feb 16 14:44:06 2015 +0000
Author: John Michelau <1005146@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 2b2600b6_d07782b6
UUID: 2b17e08c_e541d67c
Bytes: 225
We went with only 8 bits, since we figured past 255 drops the precision just wasn't interesting anymore.  Our perspective is that of somebody analyzing the log content, though, not profiling the system for logging throughput.

